{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled7.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"e7dGoCyn3HI9","colab_type":"code","colab":{}},"cell_type":"code","source":["from statistics import mean\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import os\n","import csv\n","import numpy as np\n","\n","SCORES_CSV_PATH = \"scores.csv\"\n","SCORES_PNG_PATH = \"scores.png\"\n","SOLVED_CSV_PATH = \"solved.csv\"\n","SOLVED_PNG_PATH = \"solved.png\"\n","AVERAGE_SCORE_TO_SOLVE = 195\n","CONSECUTIVE_RUNS_TO_SOLVE = 100\n","\n","\n","class ScoreLogger:\n","\n","    def __init__(self, env_name):\n","        self.scores = deque(maxlen=CONSECUTIVE_RUNS_TO_SOLVE)\n","        self.env_name = env_name\n","\n","        if os.path.exists(SCORES_PNG_PATH):\n","            os.remove(SCORES_PNG_PATH)\n","        if os.path.exists(SCORES_CSV_PATH):\n","            os.remove(SCORES_CSV_PATH)\n","\n","    def add_score(self, score, run):\n","        self._save_csv(SCORES_CSV_PATH, score)\n","        self._save_png(input_path=SCORES_CSV_PATH,\n","                       output_path=SCORES_PNG_PATH,\n","                       x_label=\"runs\",\n","                       y_label=\"scores\",\n","                       average_of_n_last=CONSECUTIVE_RUNS_TO_SOLVE,\n","                       show_goal=True,\n","                       show_trend=True,\n","                       show_legend=True)\n","        self.scores.append(score)\n","        mean_score = mean(self.scores)\n","        print (\"Scores: (min: {}, avg: {}, max: {})\\n\".format(min(self.scores),mean_score,max(self.scores)))\n","        if mean_score >= AVERAGE_SCORE_TO_SOLVE and len(self.scores) >= CONSECUTIVE_RUNS_TO_SOLVE:\n","            solve_score = run-CONSECUTIVE_RUNS_TO_SOLVE\n","            print(\"solved in {} runs,{} total runs\".format(solve_score,run))\n","            self._save_csv(SOLVED_CSV_PATH, solve_score)\n","            self._save_png(input_path=SOLVED_CSV_PATH,\n","                           output_path=SOLVED_PNG_PATH,\n","                           x_label=\"trials\",\n","                           y_label=\"steps before solve\",\n","                           average_of_n_last=None,\n","                           show_goal=False,\n","                           show_trend=False,\n","                           show_legend=False)\n","            exit()\n","\n","    def _save_png(self, input_path, output_path, x_label, y_label, average_of_n_last, show_goal, show_trend, show_legend):\n","        x = []\n","        y = []\n","        with open(input_path, \"r\") as scores:\n","            reader = csv.reader(scores)\n","            data = list(reader)\n","            for i in range(0, len(data)):\n","                x.append(int(i))\n","                y.append(int(data[i][0]))\n","\n","        plt.subplots()\n","        plt.plot(x, y, label=\"score per run\")\n","\n","        average_range = average_of_n_last if average_of_n_last is not None else len(x)\n","        plt.plot(x[-average_range:], [np.mean(y[-average_range:])] * len(y[-average_range:]), linestyle=\"--\", label=\"last {} runs average\".format(average_range))\n","\n","        if show_goal:\n","            plt.plot(x, [AVERAGE_SCORE_TO_SOLVE] * len(x), linestyle=\":\", label=\"{} score average goal\".format(AVERAGE_SCORE_TO_SOLVE))\n","\n","        if show_trend and len(x) > 1:\n","            trend_x = x[1:]\n","            z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n","            p = np.poly1d(z)\n","            plt.plot(trend_x, p(trend_x), linestyle=\"-.\",  label=\"trend\")\n","\n","        plt.title(self.env_name)\n","        plt.xlabel(x_label)\n","        plt.ylabel(y_label)\n","\n","        if show_legend:\n","            plt.legend(loc=\"upper left\")\n","\n","        plt.savefig(output_path, bbox_inches=\"tight\")\n","        plt.close()\n","\n","    def _save_csv(self, path, score):\n","        if not os.path.exists(path):\n","            with open(path, \"w\"):\n","                pass\n","        scores_file = open(path, \"a\")\n","        with scores_file:\n","            writer = csv.writer(scores_file)\n","            writer.writerow([score])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j6bZDmiq3c7G","colab_type":"code","colab":{}},"cell_type":"code","source":["import random\n","import gym\n","import numpy as np\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","\n","ENV_NAME = \"Acrobot-v1\"\n","\n","GAMMA = 0.95\n","LEARNING_RATE = 0.001\n","\n","MEMORY_SIZE = 1000000\n","BATCH_SIZE = 20\n","\n","EXPLORATION_MAX = 1.0\n","EXPLORATION_MIN = 0.01\n","EXPLORATION_DECAY = 0.995\n","\n","\n","class DQNSolver:\n","\n","    def __init__(self, observation_space, action_space):\n","        self.exploration_rate = EXPLORATION_MAX\n","\n","        self.action_space = action_space\n","        self.memory = deque(maxlen=MEMORY_SIZE)\n","\n","        self.model = Sequential()\n","        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n","        self.model.add(Dense(24, activation=\"relu\"))\n","        self.model.add(Dense(self.action_space, activation=\"linear\"))\n","        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if np.random.rand() < self.exploration_rate:\n","            return random.randrange(self.action_space)-1\n","        q_values = self.model.predict(state)\n","        return np.argmax(q_values[0])-1\n","\n","    def experience_replay(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","        batch = random.sample(self.memory, BATCH_SIZE)\n","        for state, action, reward, state_next, terminal in batch:\n","            q_update = reward\n","            if not terminal:\n","                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n","            q_values = self.model.predict(state)\n","            q_values[0][action] = q_update\n","            self.model.fit(state, q_values, verbose=0)\n","        self.exploration_rate *= EXPLORATION_DECAY\n","        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n","\n","\n","def cartpole():\n","    env = gym.make(ENV_NAME)\n","    score_logger = ScoreLogger(ENV_NAME)\n","    observation_space = env.observation_space.shape[0]\n","    action_space = env.action_space.n\n","    dqn_solver = DQNSolver(observation_space, action_space)\n","    run = 0\n","    f=0\n","    while True:\n","        run += 1\n","        state = env.reset()\n","        state = np.reshape(state, [1, observation_space])\n","        step = 0\n","        if(run%50==0 and run>0):\n","          f+=1\n","          dqn_solver.model.save_weights('dqnac-%d.h5'%(f))\n","        while True:\n","            step += 1\n","            #env.render()\n","            action = dqn_solver.act(state)\n","            state_next, reward, terminal, info = env.step(action)\n","#             reward = reward if not terminal else -reward\n","            state_next = np.reshape(state_next, [1, observation_space])\n","            dqn_solver.remember(state, action+1, reward, state_next, terminal)\n","            state = state_next\n","            if terminal:\n","                print (\"Run: %f, exploration: %f, score: %f\"%(run,dqn_solver.exploration_rate,step))\n","                score_logger.add_score(step, run)\n","                break\n","            dqn_solver.experience_replay()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4mY7Quol3nw7","colab_type":"code","colab":{}},"cell_type":"code","source":["cartpole()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dwsnIB81-doW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"423e013d-e059-4f8b-a205-2f273656cf8d","executionInfo":{"status":"ok","timestamp":1549076959453,"user_tz":-330,"elapsed":9168,"user":{"displayName":"abhay ira","photoUrl":"","userId":"16936421234120284275"}}},"cell_type":"code","source":["win =0\n","g=0\n","env = gym.make(ENV_NAME)\n","dqn_solver = DQNSolver(6, 3)\n","dqn_solver.model.load_weights('dqnac-1.h5')\n","\n","for each_game in range(100):\n","\tg+=1\n","\tpo = env.reset()\n","\tfor _ in range(500):\n","\t\t# env.render()\n","\t\t\n","\t\taction = dqn_solver.act(po)\n","\t\t\t\t\n","\t\tnew_observation, reward, done, info = env.step(action)\n","\t\tpo = new_observation\n","\n","\t\tif done:\n","\t\t\tif _!=499:\n","\t\t\t\twin+=1\n","# \t\t\tprint(each_game)\n","\t\t\tbreak\n","\n","print(win/g)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]}]}